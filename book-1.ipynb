{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv').fillna('undef')\n",
    "test = pd.read_csv('test.csv').fillna('undef').drop('id', axis=1)\n",
    "X = train\n",
    "y = train['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_second_tag(df):\n",
    "    col1 = []\n",
    "    col2 = []\n",
    "    for x in df['keyword']:\n",
    "        try:\n",
    "            smth = x.split(\"%20\")\n",
    "            col2.append(smth[1])\n",
    "            col1.append(smth[0])\n",
    "        except:\n",
    "            col2.append(\"undef\")\n",
    "            col1.append(x)\n",
    "    df.drop('keyword', axis=1, inplace = True)\n",
    "    df['keyword_one'] = col1\n",
    "    df['keyword_two'] = col2\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = separate_second_tag(X)\n",
    "test = separate_second_tag(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "\n",
    "mispell_dict = {\"aren't\" : \"are not\",\n",
    "\"can't\" : \"cannot\",\n",
    "\"couldn't\" : \"could not\",\n",
    "\"couldnt\" : \"could not\",\n",
    "\"didn't\" : \"did not\",\n",
    "\"doesn't\" : \"does not\",\n",
    "\"doesnt\" : \"does not\",\n",
    "\"don't\" : \"do not\",\n",
    "\"hadn't\" : \"had not\",\n",
    "\"hasn't\" : \"has not\",\n",
    "\"haven't\" : \"have not\",\n",
    "\"havent\" : \"have not\",\n",
    "\"he'd\" : \"he would\",\n",
    "\"he'll\" : \"he will\",\n",
    "\"he's\" : \"he is\",\n",
    "\"i'd\" : \"I would\",\n",
    "\"i'd\" : \"I had\",\n",
    "\"i'll\" : \"I will\",\n",
    "\"i'm\" : \"I am\",\n",
    "\"isn't\" : \"is not\",\n",
    "\"it's\" : \"it is\",\n",
    "\"it'll\":\"it will\",\n",
    "\"i've\" : \"I have\",\n",
    "\"let's\" : \"let us\",\n",
    "\"mightn't\" : \"might not\",\n",
    "\"mustn't\" : \"must not\",\n",
    "\"shan't\" : \"shall not\",\n",
    "\"she'd\" : \"she would\",\n",
    "\"she'll\" : \"she will\",\n",
    "\"she's\" : \"she is\",\n",
    "\"shouldn't\" : \"should not\",\n",
    "\"shouldnt\" : \"should not\",\n",
    "\"that's\" : \"that is\",\n",
    "\"thats\" : \"that is\",\n",
    "\"there's\" : \"there is\",\n",
    "\"theres\" : \"there is\",\n",
    "\"they'd\" : \"they would\",\n",
    "\"they'll\" : \"they will\",\n",
    "\"they're\" : \"they are\",\n",
    "\"theyre\":  \"they are\",\n",
    "\"they've\" : \"they have\",\n",
    "\"we'd\" : \"we would\",\n",
    "\"we're\" : \"we are\",\n",
    "\"weren't\" : \"were not\",\n",
    "\"we've\" : \"we have\",\n",
    "\"what'll\" : \"what will\",\n",
    "\"what're\" : \"what are\",\n",
    "\"what's\" : \"what is\",\n",
    "\"what've\" : \"what have\",\n",
    "\"where's\" : \"where is\",\n",
    "\"who'd\" : \"who would\",\n",
    "\"who'll\" : \"who will\",\n",
    "\"who're\" : \"who are\",\n",
    "\"who's\" : \"who is\",\n",
    "\"who've\" : \"who have\",\n",
    "\"won't\" : \"will not\",\n",
    "\"wouldn't\" : \"would not\",\n",
    "\"you'd\" : \"you would\",\n",
    "\"you'll\" : \"you will\",\n",
    "\"you're\" : \"you are\",\n",
    "\"you've\" : \"you have\",\n",
    "\"'re\": \" are\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'll\":\" will\",\n",
    "\"didn't\": \"did not\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_typical_misspell(text):\n",
    "    mispellings_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n",
    "    def replace(match):\n",
    "        return mispell_dict[match.group(0)]\n",
    "    return mispellings_re.sub(replace, text)\n",
    "\n",
    "def text_cleaning(ln):\n",
    "    corpus = []\n",
    "    wordnet = WordNetLemmatizer()\n",
    "    for text in ln:\n",
    "        text = replace_typical_misspell(text)\n",
    "        text = ' '.join(text.split('.'))\n",
    "        text = re.sub(r'\\s+', ' ', re.sub('[^A-Za-z0-9]', ' ', text.strip().lower())).strip()\n",
    "        text = re.sub(r'\\W+', ' ', text.strip().lower()).strip()\n",
    "        text = [wordnet.lemmatize(word) for word in text.split(\" \") if word not in set(stopwords.words('english'))]\n",
    "        text = ' '.join(text)\n",
    "        corpus.append(text)\n",
    "        pass\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['text'] = text_cleaning(list(X['text']))\n",
    "test['text'] = text_cleaning(list(test['text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['location'] = text_cleaning(list(X['location']))\n",
    "test['location'] = text_cleaning(list(test['location']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#location\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "\n",
    "def location_detection(ln):\n",
    "    bins = []\n",
    "    for text in ln:\n",
    "        doc = nlp(text)\n",
    "        entities = []\n",
    "        for ent in doc.ents:\n",
    "            entities.append(ent)\n",
    "        if len(entities)>0:\n",
    "            bins.append(1)\n",
    "        else:\n",
    "            bins.append(0)\n",
    "    return bins\n",
    "\n",
    "X['location_legit'] = location_detection(X['location'])\n",
    "test['location_legit'] = location_detection(test['location'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "disaster_tweets =' '.join(train[train['target'] == 1]['text'].tolist())\n",
    "non_disaster_tweets = ' '.join(train[train['target'] == 0]['text'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "def return_top_words(text,n = 10):\n",
    "    allWords = nltk.tokenize.word_tokenize(text)\n",
    "    allWordExceptStopDist = nltk.FreqDist(w.lower() for w in allWords if w not in set(stopwords.words('english')))  \n",
    "    mostCommontuples= allWordExceptStopDist.most_common(n)\n",
    "    mostCommon = [tupl[0] for tupl in mostCommontuples]\n",
    "    return mostCommon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_50_disaster_words = return_top_words(disaster_tweets,50)\n",
    "top_50_nondisaster_words = return_top_words(non_disaster_tweets,50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_500_disaster_words = return_top_words(disaster_tweets,500)\n",
    "top_500_nondisaster_words = return_top_words(non_disaster_tweets,500)\n",
    "top_disaster_exclusive = list(set(top_500_disaster_words[0]).difference(set(top_500_nondisaster_words[0])))\n",
    "top_nondisaster_exclusive = list(set(top_500_nondisaster_words[0]).difference(set(top_500_disaster_words[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total_vocab = top_disaster_exclusive + top_nondisaster_exclusive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word in total_vocab:\n",
    "    X['Is_'+word+'_present'] = X['text'].apply(lambda x: (word in x)*1)\n",
    "    test['Is_'+word+'_present'] = test['text'].apply(lambda x: (word in x)*1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>keyword_one</th>\n",
       "      <th>keyword_two</th>\n",
       "      <th>location_legit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>undef</td>\n",
       "      <td>deed reason earthquake may allah forgive u</td>\n",
       "      <td>1</td>\n",
       "      <td>undef</td>\n",
       "      <td>undef</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>undef</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>1</td>\n",
       "      <td>undef</td>\n",
       "      <td>undef</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>undef</td>\n",
       "      <td>resident asked shelter place notified officer ...</td>\n",
       "      <td>1</td>\n",
       "      <td>undef</td>\n",
       "      <td>undef</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>undef</td>\n",
       "      <td>13 000 people receive wildfire evacuation orde...</td>\n",
       "      <td>1</td>\n",
       "      <td>undef</td>\n",
       "      <td>undef</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>undef</td>\n",
       "      <td>got sent photo ruby alaska smoke wildfire pour...</td>\n",
       "      <td>1</td>\n",
       "      <td>undef</td>\n",
       "      <td>undef</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>10869</td>\n",
       "      <td>undef</td>\n",
       "      <td>two giant crane holding bridge collapse nearby...</td>\n",
       "      <td>1</td>\n",
       "      <td>undef</td>\n",
       "      <td>undef</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>10870</td>\n",
       "      <td>undef</td>\n",
       "      <td>aria ahrary thetawniest control wild fire cali...</td>\n",
       "      <td>1</td>\n",
       "      <td>undef</td>\n",
       "      <td>undef</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>10871</td>\n",
       "      <td>undef</td>\n",
       "      <td>m1 94 01 04 utc 5km volcano hawaii http co zdt...</td>\n",
       "      <td>1</td>\n",
       "      <td>undef</td>\n",
       "      <td>undef</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>10872</td>\n",
       "      <td>undef</td>\n",
       "      <td>police investigating e bike collided car littl...</td>\n",
       "      <td>1</td>\n",
       "      <td>undef</td>\n",
       "      <td>undef</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>10873</td>\n",
       "      <td>undef</td>\n",
       "      <td>latest home razed northern california wildfire...</td>\n",
       "      <td>1</td>\n",
       "      <td>undef</td>\n",
       "      <td>undef</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id location                                               text  \\\n",
       "0         1    undef         deed reason earthquake may allah forgive u   \n",
       "1         4    undef              forest fire near la ronge sask canada   \n",
       "2         5    undef  resident asked shelter place notified officer ...   \n",
       "3         6    undef  13 000 people receive wildfire evacuation orde...   \n",
       "4         7    undef  got sent photo ruby alaska smoke wildfire pour...   \n",
       "...     ...      ...                                                ...   \n",
       "7608  10869    undef  two giant crane holding bridge collapse nearby...   \n",
       "7609  10870    undef  aria ahrary thetawniest control wild fire cali...   \n",
       "7610  10871    undef  m1 94 01 04 utc 5km volcano hawaii http co zdt...   \n",
       "7611  10872    undef  police investigating e bike collided car littl...   \n",
       "7612  10873    undef  latest home razed northern california wildfire...   \n",
       "\n",
       "      target keyword_one keyword_two  location_legit  \n",
       "0          1       undef       undef               0  \n",
       "1          1       undef       undef               0  \n",
       "2          1       undef       undef               0  \n",
       "3          1       undef       undef               0  \n",
       "4          1       undef       undef               0  \n",
       "...      ...         ...         ...             ...  \n",
       "7608       1       undef       undef               0  \n",
       "7609       1       undef       undef               0  \n",
       "7610       1       undef       undef               0  \n",
       "7611       1       undef       undef               0  \n",
       "7612       1       undef       undef               0  \n",
       "\n",
       "[7613 rows x 7 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>keyword_one</th>\n",
       "      <th>keyword_two</th>\n",
       "      <th>location_legit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>undef</td>\n",
       "      <td>deed reason earthquake may allah forgive u</td>\n",
       "      <td>1</td>\n",
       "      <td>undef</td>\n",
       "      <td>undef</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>undef</td>\n",
       "      <td>forest fire near la ronge sask canada</td>\n",
       "      <td>1</td>\n",
       "      <td>undef</td>\n",
       "      <td>undef</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>undef</td>\n",
       "      <td>resident asked shelter place notified officer ...</td>\n",
       "      <td>1</td>\n",
       "      <td>undef</td>\n",
       "      <td>undef</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>undef</td>\n",
       "      <td>13 000 people receive wildfire evacuation orde...</td>\n",
       "      <td>1</td>\n",
       "      <td>undef</td>\n",
       "      <td>undef</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>undef</td>\n",
       "      <td>got sent photo ruby alaska smoke wildfire pour...</td>\n",
       "      <td>1</td>\n",
       "      <td>undef</td>\n",
       "      <td>undef</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7608</th>\n",
       "      <td>10869</td>\n",
       "      <td>undef</td>\n",
       "      <td>two giant crane holding bridge collapse nearby...</td>\n",
       "      <td>1</td>\n",
       "      <td>undef</td>\n",
       "      <td>undef</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7609</th>\n",
       "      <td>10870</td>\n",
       "      <td>undef</td>\n",
       "      <td>aria ahrary thetawniest control wild fire cali...</td>\n",
       "      <td>1</td>\n",
       "      <td>undef</td>\n",
       "      <td>undef</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7610</th>\n",
       "      <td>10871</td>\n",
       "      <td>undef</td>\n",
       "      <td>m1 94 01 04 utc 5km volcano hawaii http co zdt...</td>\n",
       "      <td>1</td>\n",
       "      <td>undef</td>\n",
       "      <td>undef</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7611</th>\n",
       "      <td>10872</td>\n",
       "      <td>undef</td>\n",
       "      <td>police investigating e bike collided car littl...</td>\n",
       "      <td>1</td>\n",
       "      <td>undef</td>\n",
       "      <td>undef</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7612</th>\n",
       "      <td>10873</td>\n",
       "      <td>undef</td>\n",
       "      <td>latest home razed northern california wildfire...</td>\n",
       "      <td>1</td>\n",
       "      <td>undef</td>\n",
       "      <td>undef</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7613 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id location                                               text  \\\n",
       "0         1    undef         deed reason earthquake may allah forgive u   \n",
       "1         4    undef              forest fire near la ronge sask canada   \n",
       "2         5    undef  resident asked shelter place notified officer ...   \n",
       "3         6    undef  13 000 people receive wildfire evacuation orde...   \n",
       "4         7    undef  got sent photo ruby alaska smoke wildfire pour...   \n",
       "...     ...      ...                                                ...   \n",
       "7608  10869    undef  two giant crane holding bridge collapse nearby...   \n",
       "7609  10870    undef  aria ahrary thetawniest control wild fire cali...   \n",
       "7610  10871    undef  m1 94 01 04 utc 5km volcano hawaii http co zdt...   \n",
       "7611  10872    undef  police investigating e bike collided car littl...   \n",
       "7612  10873    undef  latest home razed northern california wildfire...   \n",
       "\n",
       "      target keyword_one keyword_two  location_legit  \n",
       "0          1       undef       undef               0  \n",
       "1          1       undef       undef               0  \n",
       "2          1       undef       undef               0  \n",
       "3          1       undef       undef               0  \n",
       "4          1       undef       undef               0  \n",
       "...      ...         ...         ...             ...  \n",
       "7608       1       undef       undef               0  \n",
       "7609       1       undef       undef               0  \n",
       "7610       1       undef       undef               0  \n",
       "7611       1       undef       undef               0  \n",
       "7612       1       undef       undef               0  \n",
       "\n",
       "[7613 rows x 7 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tf_idf = TfidfVectorizer(ngram_range=(1, 3),\n",
    "                         binary=True,\n",
    "                         max_features = 5000,\n",
    "                         smooth_idf=False)\n",
    "X_tfidf = tf_idf.fit_transform(X['text'])\n",
    "test_tfidf = tf_idf.transform(test['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_location = TfidfVectorizer(ngram_range = (1,2),\n",
    "                              binary = True,\n",
    "                              max_features = 1500,\n",
    "                              smooth_idf = False)\n",
    "location_X_tfidf = tf_location.fit_transform(X['location'])\n",
    "location_test_tfidf = tf_location.transform(test['location'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.concat([X,\n",
    "                        pd.DataFrame(X_tfidf.toarray(),\n",
    "                                     columns = ['text_contains_'+ str(text) for text in tf_idf.get_feature_names()]),\n",
    "                        pd.DataFrame(location_X_tfidf.toarray(),\n",
    "                                     columns = ['location_contains_'+str(text) for text in tf_location.get_feature_names()])],axis = 1)\n",
    "test_data = pd.concat([test,\n",
    "                       pd.DataFrame(test_tfidf.toarray(),\n",
    "                                    columns = ['text_contains_'+ str(text) for text in tf_idf.get_feature_names()]),\n",
    "                       pd.DataFrame(location_test_tfidf.toarray(),\n",
    "                                    columns = ['location_contains_'+str(text) for text in tf_location.get_feature_names()])],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vec(dataframe):\n",
    "    texts = dataframe['text'].tolist()\n",
    "    vectors = []\n",
    "    for doc in nlp.pipe(texts):\n",
    "        vectors.append(list(doc.vector))\n",
    "    df = pd.DataFrame(vectors,columns = ['vec_'+str(i) for i in range(300)])\n",
    "    return df\n",
    "vec_train = create_vec(train_data)\n",
    "vec_test = create_vec(test_data)\n",
    "train_data = pd.concat([train_data,vec_train],axis = 1)\n",
    "test_data = pd.concat([test_data,vec_test],axis = 1)\n",
    "\n",
    "train_data = train_data.drop(['keyword_one', 'keyword_two','location','text'],axis = 1)\n",
    "test_data = test_data.drop(['keyword_one', 'keyword_two','location','text'],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
